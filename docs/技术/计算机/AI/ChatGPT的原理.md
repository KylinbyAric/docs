# ChatGpt(generative pre-trained Transformer)是什么？

一句话概括就是一个读过全互联网的“超级语言模仿大师”。人类回答问题/构思时往往是先有一个概念，然后再借助语言工具来生成答案。但大模型却是根据已有字来生成下一个字，这并不是先有答案再回答的模式。

# 核心原理

分为三步：
1、预训练：学习各种互联网上的知识(包括假的)
2、微调：对部分功能进行专项训练
3、应用：将模型应用到实际问题中

# 三大核心技术

1、Transformer：Transformer 是一种基于注意力机制的模型，它使用了多个编码器（encoder）和多个解码器（decoder）来处理输入和输出。Transformer 模型在自然语言处理（NLP）领域被广泛使用，因为它的编码和解码过程非常简单，并且可以处理长序列。
2、注意力机制：Attention 是一种机制，它可以帮助模型在处理输入时，根据输入的每个元素对输出进行加权，从而实现对输入的关注。在 Transformer 模型中，Attention 被用于编码和解码过程中，以实现对输入的关注。
3、神经网络：Neural Network 是一种计算模型，它由多个神经元组成，每个神经元都有自己的权重和偏置，以及一个激活函数。在 Transformer 模型中，神经网络被用于编码和解码过程中。

## 神经网络

核心作用是把碎片信息组合成复杂决策‌，像人类大脑的神经元传递信号。

### 前向传播

‌把输入数据（如像素/文字）逐层转换为高级特征‌，类似瀑布水流经过多级滤网。
从输入层到隐藏层，从隐藏层到输出层
最终得出一个概率数值

### 反向传播

‌比较输出结果与正确答案的误差，从后往前逐层调整权重‌，类似调校收音机旋钮直到杂音消失。
目标‌：让输出 y = 0.55 更接近正确答案（假设真实概率应为0.9）
‌步骤1：计算输出层误差
步骤2：误差反向分配‌（关键！）
步骤3：更新输出层权重

### 损失函数

神经网络的奖励措施(纠正措施)

### 感知机

将一个输入按照不同维度(特征)进行拆解，得到f(x)=w1*f1(x)+w2*f2(x)+w3*f3(x)+w4*f4(x)+w5*f5(x)+w6*f6(x)+w7*f7(x)+w8*f8(x)+w9*f9(x)-b
每一个fn(x)都是一个特征，每个wn是一个权重(参数)，b则是目标得分，最终计算出来的一个f(x)的值，如果>0则激活，<0则不激活

如果是异或操作，则感知机无法找到这么一个fx.
ps:当时神经网络被这么一个小小反例打入冷宫40年，但辛顿却依然坚持着神经网络。人们因感知机而充满幻想，又因异或反例而抛弃感知机。

### MLP(多层感知机)

每层的感知机的输入是下一层感知机的输出。这样，只要层数足够深，宽度足够宽，那MLP则能模拟任何函数

### CNN(卷积神经网络)

在MLP里，每一层的感知机要连接上一层的所有感知机，层数加1或者宽度加1，则参数数量指数增长。如果每层的感知机只与相邻的上一层的感知机相连接，连接强度也都类似，
那么就能极大地减少计算量。

### ResNet(残差网络)

在CNN里，如果层数太多了，计算量也是极大的。有没有更好的模型呢？

‌残差连接‌：前面的感知机都是只与相连的一层连接，如果能跳跃式地连接，是不是就能减少计算量呢？

训练模型，本质上是给出一堆二维空间里的点，然后让模型调整参数，找到一个拟合函数。如何度量拟合函数和真实函数之间的差距呢？答案是损失函数。

### 拟合函数和损失函数

通过模型训练的出的拟合函数，简单理解就是一个多项式fx=k1x^1+k2x^2+k3x^3+k4x^4+k5x^5+k6x^6+k7x^7+k8x^8.....
拟合函数和真实函数之间的差距(方差)，就是损失函数。每个拟合函数上的点于样本点的距离差的平方相加，这就是损失函数的输出

### 梯度下降

如何更快地找到参数呢？答案是梯度下降。
梯度：给定某个曲面的位置(x,y),函数值变化最快的方向，也是曲面在局部最陡峭的方向。也即二维版本的求导(偏导数)。
只要我们根据该梯度方向，同时调整两个参数，就能愉快地找到两个最优的参数。这就是梯度下降。

### 反向传播

那怎么计算出如此多维度下的梯度呢？答案是反向传播。

### 复合函数的求导

x->g(x)->f(g(x)),这样就得到一个复合函数
怎么求出复合函数对x的求导呢？
1、先求出g(x)的导数，即g'(x)
2、再求出f(g(x))的导数，即f'(g(x))
3、将两个导数相乘，即f'(g(x))*g'(x).(为什么是相乘？因为g'(x)是g(x)的导数，g'(x)乘以f'(g(x))，就等于f(g(x))的导数)

## 注意力机制

‌从废话中精准抓重点‌，像人类聊天时自动忽略噪音。

## Transformer

‌同时处理整段话‌（传统AI只能逐字读），极大提升效率。
