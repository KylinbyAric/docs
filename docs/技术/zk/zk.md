Zookeeper



1、分布式锁
	原理是client1创建一个临时节点 create -e /lock.   另外一个client2同样适用create -e /lock时会提示已经存在，client2 监听这个path. stat -w /lock.如果client1 宕机或者quit，则client2会收到监听事件，再使用create -e /lock能创建成功。


CAP理论：
1、C 一致性 多个节点的数据在同一时间都是一致的
2、A可用性 多个节点什么时候都可以正常访问
3、P分区容错性 一个节点出问题了，其他节点还可以正常提供服务

为什么不能兼容三项？
根本原因是数据同步时会出现网络问题，比如A节点更新了数据，在A节点同步数据到B节点时网络出了问题。假设分区容错性可行，那么如果要保证一致性，那么节点B就必须等待网络恢复，数据更新成最新，此时节点B是一直堵塞的，违背了可用性。假设保证可用性，即允许节点B返回旧数据，那么这又违背了一致性。所以三者在网络出问题时是不可能同时存在。除非网络永不堵塞，且实时性很高，但这又怎么可能呢？

取舍：
1、舍去分区容错性，咳咳，这跟不是分布式有什么区别呢？
2、舍去一致性，那就可能出现访问的是旧数据的问题了。常用的是最终一致性即可
3、舍去可用性，那就会出现某一个或多个节点服务不可用。

咳咳，所以还是舍去一致性比较香，影响最小。但银行除外，毕竟是钱嘛:)

两种分布式一致性算法：raft和paxos

paxos算法：
1、preprare阶段
	leader发送prepare(A)给paxos集群，绝大部分(>1/2)返回prepare(A)_ok，则进行下一步，否则重新发起。
2、accepted
	leader发送accepted(A)给paxos集群，绝大部分(>1/2)返回accepted(A)_ok,则确定最终值为A。

Raft算法：
1、三个角色：
leader：集群对外的角色，所有的写都是由leader分发给集群的
follower：只接受leader的命令
 codidate：竞选leader

选举过程：
如果一个follower好久没收到leader，那么有三种情况
1、还没选出leader，此时正在选举
2、leader挂了
3、leader和该follower网络障碍
此时该节点主动发起选举，步骤如下：
1、增加本地的current term(当前任期),切换为candidate
2、投自己一票
3、并行向其他节点发送RequestVote RPCs
4、等待其他节点回复

等待时根据收到的消息，可能出现三个结果
1、收到大部分票，成为leader
2、被告知别人已经当选，自行切换到follower
3、很久没收到大部分票，则重新发起选举

第一种情况：
向所有的节点发送消息。
投票约束：
1、每个任期只有一票
2、候选人知道信息肯定比follower多（？）
3、谁选举消息先到选谁

第二种情况：
收到leader的心跳消息，得知leader的term不低于自己的term,则知道已经有leader了，自动切换为follower

第三种：
等到本身的随机时间超时后重新选举，延长系统不可用时间（因为没leader不能处理写请求）。所以为了避免平票，节点的数量都是奇数个的。



candidate像集群发起投票，所有节点投票，如果candidate收到的票数>n/2 +1则成为leader。
如果两个candidate在timeOut时间内得到同样的票数时，那么再过timeOut之后再发起投票，首先发出票的候选者得到大多数票，成为leader，因为follower这时只能投一个，所以后一个候选者将落选成为follower

日志复制：


PathChildrenCache:
1、无法监听本节点的数据变化
2、只能坚挺下一级的数据变化

脑裂
假如有6个节点，3个节点在机房，另外3个在另一个机房，现在机房之间通信突然断开。那么两个机房会进行选举，然后会选举出两个leader，等通信放开后，集群
就出现了两个leader了。
解决办法就是过半机制，过半就是大于一半的节点同意才会成为leader。如上例子，如果只是等于一半的节点同意就会出现脑裂。这样要么无法选出leader，要么只有
一个leader


